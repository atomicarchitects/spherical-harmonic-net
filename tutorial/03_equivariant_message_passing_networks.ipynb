{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E(3)-Equivariant Message-Passing Networks\n",
    "\n",
    "Here, we briefly introduce E(3)-equivariant message-passing networks.\n",
    "We implement a simple version of [NequIP](https://www.nature.com/articles/s41467-022-29939-5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from typing import Callable, Tuple, Sequence\n",
    "import flax.linen as nn\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jraph\n",
    "import e3nn_jax as e3nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"A simple multi-layer perceptron.\"\"\"\n",
    "\n",
    "    output_dims: int\n",
    "    hidden_dims: int\n",
    "    num_layers: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        for _ in range(self.num_layers - 1):\n",
    "            x = nn.Dense(features=self.hidden_dims)(x)\n",
    "            x = nn.LayerNorm()(x)\n",
    "            x = nn.silu(x)\n",
    "        x = nn.Dense(features=self.output_dims)(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def compute_features_of_relative_vectors(relative_vectors: jnp.ndarray, lmax: int) -> Tuple[e3nn.IrrepsArray, jnp.ndarray]:\n",
    "    \"\"\"Compute the spherical harmonics of the relative vectors and their norms.\"\"\"\n",
    "    relative_vectors_sh = e3nn.spherical_harmonics(\n",
    "        e3nn.s2_irreps(lmax=lmax),\n",
    "        relative_vectors,\n",
    "        normalize=True,\n",
    "        normalization=\"norm\",\n",
    "    )\n",
    "    relative_vectors_norm = jnp.linalg.norm(\n",
    "        relative_vectors, axis=-1, keepdims=True\n",
    "    )\n",
    "    return relative_vectors_sh, relative_vectors_norm\n",
    "\n",
    "\n",
    "class AtomEmbedding(nn.Module):\n",
    "    \"\"\"Embeds atomic numbers into a learnable vector space.\"\"\"\n",
    "\n",
    "    embed_dims: int\n",
    "    max_atomic_number: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, atomic_numbers: jnp.ndarray) -> jnp.ndarray:\n",
    "        atom_embeddings = nn.Embed(\n",
    "            num_embeddings=self.max_atomic_number,\n",
    "            features=self.embed_dims\n",
    "        )(atomic_numbers)\n",
    "        return e3nn.IrrepsArray(f\"{self.embed_dims}x0e\", atom_embeddings)\n",
    "\n",
    "\n",
    "\n",
    "class SimpleNetworkLayer(nn.Module):\n",
    "    \"\"\"A layer of a simple E(3)-equivariant message passing network.\"\"\"\n",
    "   \n",
    "    mlp_hidden_dims: int\n",
    "    mlp_num_layers: int\n",
    "    output_irreps: e3nn.Irreps\n",
    "    tensor_product_fn: Callable[[], nn.Module]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, node_features: e3nn.IrrepsArray, senders: jnp.ndarray, receivers: jnp.ndarray, relative_vectors_sh: e3nn.IrrepsArray, relative_vectors_norm: jnp.ndarray) -> e3nn.IrrepsArray:\n",
    "        # Compute the skip connection.\n",
    "        node_features_skip = node_features\n",
    "\n",
    "        # Tensor product of the relative vectors and the neighbouring node features.\n",
    "        node_features_broadcasted = node_features[senders]\n",
    "        node_features_broadcasted = self.tensor_product_fn()(\n",
    "            relative_vectors_sh, node_features_broadcasted\n",
    "        )\n",
    "    \n",
    "        # Simply multiply each irrep by a learned scalar, based on the norm of the relative vector.\n",
    "        scalars = mlp.MLP(\n",
    "            output_dims=node_features_broadcasted.irreps.num_irreps,\n",
    "            hidden_dims=self.mlp_hidden_dims,\n",
    "            num_layers=self.mlp_num_layers\n",
    "        )(relative_vectors_norm)\n",
    "        scalars = e3nn.IrrepsArray(f\"{scalars.shape[-1]}x0e\", scalars)\n",
    "        node_features_broadcasted = jax.vmap(lambda scale, feature: scale * feature)(\n",
    "            scalars, node_features_broadcasted\n",
    "        )\n",
    "\n",
    "        # Aggregate the node features back.\n",
    "        node_features = e3nn.scatter_mean(\n",
    "            node_features_broadcasted,\n",
    "            dst=receivers,\n",
    "            output_size=node_features.shape[0],\n",
    "        )\n",
    "\n",
    "        # Apply a non-linearity.\n",
    "        # Note that using an unnormalized non-linearity will make the model not equivariant.\n",
    "        gate_irreps = e3nn.Irreps(f\"{node_features.irreps.num_irreps}x0e\")\n",
    "        node_features_expanded = e3nn.flax.Linear(\n",
    "            node_features.irreps + gate_irreps\n",
    "        )(node_features)\n",
    "        node_features = e3nn.gate(node_features_expanded)\n",
    "\n",
    "        # Add the skip connection.\n",
    "        node_features = e3nn.concatenate([node_features, node_features_skip])\n",
    "    \n",
    "        # Apply a linear transformation to the output.\n",
    "        node_features = e3nn.flax.Linear(self.output_irreps)(node_features)\n",
    "        return node_features\n",
    "\n",
    "\n",
    "\n",
    "class SimpleNetwork(nn.Module):\n",
    "    \"\"\"A simple E(3)-equivariant message passing network.\"\"\"\n",
    "\n",
    "    sh_lmax: int\n",
    "    init_embed_dims: int\n",
    "    max_atomic_number: int\n",
    "    mlp_hidden_dims: int\n",
    "    mlp_num_layers: int\n",
    "    output_irreps_per_layer: Sequence[e3nn.Irreps]\n",
    "    tensor_product_fn: Callable[[], nn.Module]\n",
    "    readout: nn.Module\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, graphs: jraph.GraphsTuple) -> jnp.ndarray:\n",
    "        # Node features are initially the atomic numbers embedded.\n",
    "        node_features = graphs.nodes[\"numbers\"]\n",
    "        node_features = AtomEmbedding(\n",
    "            embed_dims=self.init_embed_dims,\n",
    "            max_atomic_number=self.max_atomic_number,\n",
    "        )(node_features)\n",
    "\n",
    "        # Precompute the spherical harmonics of the relative vectors.\n",
    "        positions = graphs.nodes[\"positions\"]\n",
    "        relative_vectors = positions[graphs.receivers] - positions[graphs.senders]\n",
    "        relative_vectors_sh, relative_vectors_norm = compute_features_of_relative_vectors(\n",
    "            relative_vectors, lmax=self.sh_lmax,\n",
    "        )\n",
    "\n",
    "        # Message passing.\n",
    "        for output_irreps in self.output_irreps_per_layer:\n",
    "            node_features = SimpleNetworkLayer(\n",
    "                mlp_hidden_dims=self.mlp_hidden_dims,\n",
    "                mlp_num_layers=self.mlp_num_layers,\n",
    "                tensor_product_fn=self.tensor_product_fn,\n",
    "                output_irreps=output_irreps,\n",
    "            )(node_features, graphs.senders, graphs.receivers, relative_vectors_sh, relative_vectors_norm)\n",
    "\n",
    "        # Readout.\n",
    "        return self.readout(node_features, graphs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "symphony",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
