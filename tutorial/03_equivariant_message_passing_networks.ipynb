{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E(3)-Equivariant Message-Passing Networks\n",
    "\n",
    "Here, we briefly introduce E(3)-equivariant message-passing networks.\n",
    "\n",
    "A message-passing network is a neural network that operates on a graph, computing node and graph-level representations.\n",
    "\n",
    "The nodes of the graph are associated with feature vectors.\n",
    "For example, in a molecular graph, each node can represent an atom and the initial feature vector can represent the atom type embedded as a vector. This is the setting we consider in this example.\n",
    "\n",
    "The message-passing network operates in iterations.\n",
    "At each iteration of message-passing, each node aggregates messages from its neighbors and updates its own feature vector. \n",
    "\n",
    "See these [Distill](https://distill.pub/2021/gnn-intro/) [articles](https://distill.pub/2021/understanding-gnns/) for an interactive introduction to graph neural networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here, we will implement a simpler version of [NequIP](https://www.nature.com/articles/s41467-022-29939-5). The operation of the network is shown in the figure below. If this doesn't make sense to you, don't worry! We will explain the details in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](images/algorithm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from typing import Callable, Tuple, Sequence\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jraph\n",
    "import e3nn_jax as e3nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's discuss [Flax](https://github.com/google/flax), the JAX framework that we will use to implement the network.\n",
    "\n",
    "Flax allows us to define neural networks in JAX, but in a way similar to that of PyTorch.\n",
    "\n",
    "Let's see how we can define a multi-layer perceptron (MLP) in Flax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"A simple multi-layer perceptron.\"\"\"\n",
    "\n",
    "    # These are attributes.\n",
    "    output_dims: int\n",
    "    hidden_dims: int\n",
    "    num_layers: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        for _ in range(self.num_layers - 1):\n",
    "            x = nn.Dense(features=self.hidden_dims)(x)\n",
    "            x = nn.LayerNorm()(x)\n",
    "            x = nn.silu(x)\n",
    "        x = nn.Dense(features=self.output_dims)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*@nn.compact* is a decorator that allows us to define submodules (eg. nn.Dense, nn.LayerNorm above) within the __call__ method.\n",
    "\n",
    "Another thing that you may notice is the automatic shape inference: we didn't need to define the input dimensions of 'x' above. \n",
    "\n",
    "We can create a model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(output_dims=5, hidden_dims=8, num_layers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX is a functional language, which means that we need to explicitly pass in the parameters to the model when calling it. This is different from PyTorch, where the parameters are stored in the model.\n",
    "\n",
    "Fortunately, Flax provides a convenient way to handle this. We can use the 'init' method to initialize the model and the 'apply' method to call it.\n",
    "\n",
    "JAX is also explicit about randomness. We need to pass in a PRNGKey to the model when calling it, which is a seed for the random number generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[3m                                  MLP Summary                                   \u001b[0m\n",
      "┏━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mpath       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmodule   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1minputs      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutputs     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mparams              \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│             │ MLP       │ \u001b[2mfloat32\u001b[0m[3,4] │ \u001b[2mfloat32\u001b[0m[3,5] │                      │\n",
      "├─────────────┼───────────┼──────────────┼──────────────┼──────────────────────┤\n",
      "│ Dense_0     │ Dense     │ \u001b[2mfloat32\u001b[0m[3,4] │ \u001b[2mfloat32\u001b[0m[3,8] │ bias: \u001b[2mfloat32\u001b[0m[8]     │\n",
      "│             │           │              │              │ kernel: \u001b[2mfloat32\u001b[0m[4,8] │\n",
      "│             │           │              │              │                      │\n",
      "│             │           │              │              │ \u001b[1m40 \u001b[0m\u001b[1;2m(160 B)\u001b[0m           │\n",
      "├─────────────┼───────────┼──────────────┼──────────────┼──────────────────────┤\n",
      "│ LayerNorm_0 │ LayerNorm │ \u001b[2mfloat32\u001b[0m[3,8] │ \u001b[2mfloat32\u001b[0m[3,8] │ bias: \u001b[2mfloat32\u001b[0m[8]     │\n",
      "│             │           │              │              │ scale: \u001b[2mfloat32\u001b[0m[8]    │\n",
      "│             │           │              │              │                      │\n",
      "│             │           │              │              │ \u001b[1m16 \u001b[0m\u001b[1;2m(64 B)\u001b[0m            │\n",
      "├─────────────┼───────────┼──────────────┼──────────────┼──────────────────────┤\n",
      "│ Dense_1     │ Dense     │ \u001b[2mfloat32\u001b[0m[3,8] │ \u001b[2mfloat32\u001b[0m[3,5] │ bias: \u001b[2mfloat32\u001b[0m[5]     │\n",
      "│             │           │              │              │ kernel: \u001b[2mfloat32\u001b[0m[8,5] │\n",
      "│             │           │              │              │                      │\n",
      "│             │           │              │              │ \u001b[1m45 \u001b[0m\u001b[1;2m(180 B)\u001b[0m           │\n",
      "├─────────────┼───────────┼──────────────┼──────────────┼──────────────────────┤\n",
      "│\u001b[1m \u001b[0m\u001b[1m           \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m         \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m            \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m       Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m101 \u001b[0m\u001b[1;2m(404 B)\u001b[0m\u001b[1m         \u001b[0m\u001b[1m \u001b[0m│\n",
      "└─────────────┴───────────┴──────────────┴──────────────┴──────────────────────┘\n",
      "\u001b[1m                                                                                \u001b[0m\n",
      "\u001b[1m                         Total Parameters: 101 \u001b[0m\u001b[1;2m(404 B)\u001b[0m\u001b[1m                          \u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = jnp.ones((3, 4))\n",
    "rng = jax.random.PRNGKey(0)\n",
    "\n",
    "# Initialize the model.\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "params = model.init(init_rng, x)\n",
    "\n",
    "# Flax provides a convenient way to print the model structure,\n",
    "# inputs and outputs.\n",
    "print(nn.tabulate(model, init_rng)(x))\n",
    "\n",
    "# The forward pass is just obtained by calling model.apply.\n",
    "y = model.apply(params, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not go too much into the details of Flax here, but you can find more information in the [Flax documentation](https://flax.readthedocs.io/en/latest/) with some cool [examples](https://flax.readthedocs.io/en/latest/examples/index.html).\n",
    "\n",
    "Let's move on to the implementation of the E(3)-equivariant message-passing network.\n",
    "\n",
    "Initially, each node gets a feature vector based on the atom type. The 'AtomEmbedding' class below implements this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtomEmbedding(nn.Module):\n",
    "    \"\"\"Embeds atomic numbers into a learnable vector space.\"\"\"\n",
    "\n",
    "    embed_dims: int\n",
    "    max_atomic_number: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, atomic_numbers: jnp.ndarray) -> jnp.ndarray:\n",
    "        atom_embeddings = nn.Embed(\n",
    "            num_embeddings=self.max_atomic_number, features=self.embed_dims\n",
    "        )(atomic_numbers)\n",
    "        return e3nn.IrrepsArray(f\"{self.embed_dims}x0e\", atom_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the feature vectors of each node are initially scalars, because the atom types (and hence, their embeddings) are invariant under rotations.\n",
    "\n",
    "The next step is to convolve the feature vectors of the neighbors of each node to get a new feature vector for each node. This is done multiple times, so we define a single 'SimpleNetworkLayer' that we can use multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNetworkLayer(nn.Module):\n",
    "    \"\"\"A layer of a simple E(3)-equivariant message passing network.\"\"\"\n",
    "\n",
    "    mlp_hidden_dims: int\n",
    "    mlp_num_layers: int\n",
    "    output_irreps: e3nn.Irreps\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(\n",
    "        self,\n",
    "        node_features: e3nn.IrrepsArray,\n",
    "        senders: jnp.ndarray,\n",
    "        receivers: jnp.ndarray,\n",
    "        relative_vectors_sh: e3nn.IrrepsArray,\n",
    "        relative_vectors_norm: jnp.ndarray,\n",
    "    ) -> e3nn.IrrepsArray:\n",
    "        \n",
    "        # Compute the skip connection.\n",
    "        node_features_skip = node_features\n",
    "\n",
    "        # Tensor product of the relative vectors and the neighbouring node features.\n",
    "        node_features_broadcasted = node_features[senders]\n",
    "        node_features_broadcasted = e3nn.tensor_product(\n",
    "            relative_vectors_sh, node_features_broadcasted\n",
    "        )\n",
    "\n",
    "        # Simply multiply each irrep by a learned scalar, based on the norm of the relative vector.\n",
    "        scalars = MLP(\n",
    "            output_dims=node_features_broadcasted.irreps.num_irreps,\n",
    "            hidden_dims=self.mlp_hidden_dims,\n",
    "            num_layers=self.mlp_num_layers,\n",
    "        )(relative_vectors_norm)\n",
    "        scalars = e3nn.IrrepsArray(f\"{scalars.shape[-1]}x0e\", scalars)\n",
    "        node_features_broadcasted = jax.vmap(lambda scale, feature: scale * feature)(\n",
    "            scalars, node_features_broadcasted\n",
    "        )\n",
    "\n",
    "        # Aggregate the node features back.\n",
    "        node_features = e3nn.scatter_mean(\n",
    "            node_features_broadcasted,\n",
    "            dst=receivers,\n",
    "            output_size=node_features.shape[0],\n",
    "        )\n",
    "\n",
    "        # Apply a non-linearity.\n",
    "        # Note that using an unnormalized non-linearity will make the model not equivariant.\n",
    "        gate_irreps = e3nn.Irreps(f\"{node_features.irreps.num_irreps}x0e\")\n",
    "        node_features_expanded = e3nn.flax.Linear(node_features.irreps + gate_irreps)(\n",
    "            node_features\n",
    "        )\n",
    "        node_features = e3nn.gate(node_features_expanded)\n",
    "\n",
    "        # Add the skip connection.\n",
    "        node_features = e3nn.concatenate([node_features, node_features_skip])\n",
    "\n",
    "        # Apply a linear transformation to the output.\n",
    "        node_features = e3nn.flax.Linear(self.output_irreps)(node_features)\n",
    "        return node_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The message-passing is implemented by keeping track of senders and receiver nodes. This is a sparse representation of the adjacency matrix of the graph! The features of each neighbor are then 'tensor product'ed with the spherical harmonics of the relative position of the sender and receiver nodes. This creates higher-order irreps!\n",
    "\n",
    "* Since we use $O(3)$-equivariant tensor products and non-linearities, the network is equivariant to *rotations*.\n",
    "\n",
    "* To account for *translation symmetry*, we only use relative positions of the neighbors. This guarantees that the network is invariant to translations.\n",
    "\n",
    "* To account for *permutation symmetry* amongst the neighbors, we sum the feature vectors of the neighbors and only then apply a non-linearity.\n",
    "\n",
    "Fortunately, most of these operations are already implemented in e3nn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features_of_relative_vectors(\n",
    "    relative_vectors: jnp.ndarray, lmax: int\n",
    ") -> Tuple[e3nn.IrrepsArray, jnp.ndarray]:\n",
    "    \"\"\"Compute the spherical harmonics of the relative vectors and their norms.\"\"\"\n",
    "    relative_vectors_sh = e3nn.spherical_harmonics(\n",
    "        e3nn.s2_irreps(lmax=lmax),\n",
    "        relative_vectors,\n",
    "        normalize=True,\n",
    "        normalization=\"norm\",\n",
    "    )\n",
    "    relative_vectors_norm = jnp.linalg.norm(relative_vectors, axis=-1, keepdims=True)\n",
    "    return relative_vectors_sh, relative_vectors_norm\n",
    "\n",
    "\n",
    "class SimpleNetwork(nn.Module):\n",
    "    \"\"\"A simple E(3)-equivariant message passing network.\"\"\"\n",
    "\n",
    "    sh_lmax: int\n",
    "    init_embed_dims: int\n",
    "    max_atomic_number: int\n",
    "    mlp_hidden_dims: int\n",
    "    mlp_num_layers: int\n",
    "    output_irreps_per_layer: Sequence[e3nn.Irreps]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, graphs: jraph.GraphsTuple) -> jnp.ndarray:\n",
    "        # Node features are initially the atomic numbers embedded.\n",
    "        node_features = graphs.nodes[\"numbers\"]\n",
    "        node_features = AtomEmbedding(\n",
    "            embed_dims=self.init_embed_dims,\n",
    "            max_atomic_number=self.max_atomic_number,\n",
    "        )(node_features)\n",
    "\n",
    "        # Precompute the spherical harmonics of the relative vectors.\n",
    "        positions = graphs.nodes[\"positions\"]\n",
    "        relative_vectors = positions[graphs.receivers] - positions[graphs.senders]\n",
    "        (\n",
    "            relative_vectors_sh,\n",
    "            relative_vectors_norm,\n",
    "        ) = compute_features_of_relative_vectors(\n",
    "            relative_vectors,\n",
    "            lmax=self.sh_lmax,\n",
    "        )\n",
    "\n",
    "        # Message passing.\n",
    "        for output_irreps in self.output_irreps_per_layer:\n",
    "            node_features = SimpleNetworkLayer(\n",
    "                mlp_hidden_dims=self.mlp_hidden_dims,\n",
    "                mlp_num_layers=self.mlp_num_layers,\n",
    "                output_irreps=output_irreps,\n",
    "            )(\n",
    "                node_features,\n",
    "                graphs.senders,\n",
    "                graphs.receivers,\n",
    "                relative_vectors_sh,\n",
    "                relative_vectors_norm,\n",
    "            )\n",
    "\n",
    "        # Readout.\n",
    "        return node_features, e3nn.scatter_mean(node_features, nel=graphs.n_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we readout the graph-level representation by summing the feature vectors of all nodes, which accounts for the permutation symmetry of the nodes.\n",
    "\n",
    "In conclusion, we have implemented a simple E(3)-equivariant message-passing network that operates on a molecular graph. The differences with general message-passing networks are the use of equivariant features, but otherwise, the operations are quite similar.\n",
    "\n",
    "We end with some visualizations of $E(3)$-equivariant features from a randomly initialized $E(3)$-equivariant network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = jraph.GraphsTuple(\n",
    "    nodes=dict(\n",
    "        numbers=jnp.array([1, 6, 8]),\n",
    "        positions=jnp.array([[0.0, 1.0, 1.0], [1.0, 0.0, 0.0], [2.0, 0.0, 0.0]]),\n",
    "    ),\n",
    "    senders=jnp.array([0, 1, 2]),\n",
    "    receivers=jnp.array([1, 2, 0]),\n",
    "    globals=jnp.array([0]),\n",
    "    edges=None,\n",
    "    n_node=jnp.array([3]),\n",
    "    n_edge=jnp.array([3]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNetwork(\n",
    "    sh_lmax=3,\n",
    "    init_embed_dims=8,\n",
    "    max_atomic_number=10,\n",
    "    mlp_hidden_dims=8,\n",
    "    mlp_num_layers=2,\n",
    "    output_irreps_per_layer=[\n",
    "        e3nn.Irreps(\"8x0e + 8x1o\"),\n",
    "        e3nn.Irreps(\"8x0e + 8x1o\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "params = model.init(init_rng, graph)\n",
    "node_features, global_features = model.apply(params, graph)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "symphony",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
