{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E(3)-Equivariant Message-Passing Networks\n",
    "\n",
    "Here, we briefly introduce $E(3)$-equivariant message-passing networks.\n",
    "\n",
    "$E(3)$ is the Euclidean group in 3D, which is the group of all translations and rotations in 3D space.\n",
    "\n",
    "We have already talked about the irreps of $O(3)$, which is the subgroup of $E(3)$ that only contains rotations (and inversion).\n",
    "$E(3)$-equivariant neural networks have traditionally been built using this machinery along with translation-invariant operations. (Although recently, there have been some new developments building translation-equivariant networks such as the [Geometric Algebra Transformer](https://arxiv.org/abs/2305.18415).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from typing import Callable, Tuple, Sequence\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jraph\n",
    "import e3nn_jax as e3nn\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A message-passing network is a neural network that operates on a graph $G = (V, E)$, computing node, edge and graph representations.\n",
    "\n",
    "Often, 3D graphs are constructed from point clouds. Each point in the point cloud is a node in the graph, and edges are defined between nearby points using a radius cutoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_points = 5\n",
    "positions = jax.random.normal(jax.random.PRNGKey(0), (N_points, 3))\n",
    "\n",
    "distance_matrix = positions[:, None, :] - positions[None, :, :]\n",
    "distance_matrix = jnp.linalg.norm(distance_matrix, axis=-1)\n",
    "assert distance_matrix.shape == (N_points, N_points)\n",
    "\n",
    "senders, receivers = jnp.nonzero(distance_matrix < 2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of a molecular graph.\n",
    "graph = jraph.GraphsTuple(\n",
    "    nodes=dict(\n",
    "        positions=positions,\n",
    "        atomic_numbers=jnp.zeros(len(positions), dtype=jnp.int32),\n",
    "    ),\n",
    "    senders=senders,\n",
    "    receivers=receivers,\n",
    "    globals=None,\n",
    "    edges=None,\n",
    "    n_node=jnp.asarray([len(positions)]),\n",
    "    n_edge=jnp.asarray([len(senders)]),\n",
    ")\n",
    "\n",
    "def plot_graph(graph):\n",
    "    positions = graph.nodes[\"positions\"]\n",
    "    \n",
    "    # Draw a graph in 3D\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=positions[:, 0],\n",
    "            y=positions[:, 1],\n",
    "            z=positions[:, 2],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=12, color=graph.nodes[\"atomic_numbers\"], colorscale=\"Viridis\"),\n",
    "            name=\"Nodes\",\n",
    "        )\n",
    "    )\n",
    "    for index, (sender, receiver) in enumerate(zip(graph.senders, graph.receivers)):\n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                x=[positions[sender, 0], positions[receiver, 0]],\n",
    "                y=[positions[sender, 1], positions[receiver, 1]],\n",
    "                z=[positions[sender, 2], positions[receiver, 2]],\n",
    "                mode=\"lines\",\n",
    "                line=dict(color=\"black\"),\n",
    "                showlegend=(index == 0),\n",
    "                name=\"Edges\",\n",
    "            )\n",
    "        )\n",
    "    fig.update_layout(scene=dict(\n",
    "        aspectmode=\"cube\",\n",
    "        xaxis=dict(title=\"x\", range=[-3, 3]),\n",
    "        yaxis=dict(title=\"y\", range=[-3, 3]),\n",
    "        zaxis=dict(title=\"z\", range=[-3, 3]),\n",
    "    ))\n",
    "    return fig\n",
    "\n",
    "fig = plot_graph(graph)\n",
    "fig.update_layout(title=\"3D Graph\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nodes of the graph are associated with feature vectors.\n",
    "For example, in a molecular graph, each node can represent an atom and the initial feature vector can represent the atom type embedded as a vector. This is the setting we consider in this example.\n",
    "\n",
    "The message-passing network operates in iterations.\n",
    "At each iteration of message-passing, each node aggregates messages from its neighbors and updates its own feature vector. \n",
    "\n",
    "See these [Distill](https://distill.pub/2021/gnn-intro/) [articles](https://distill.pub/2021/understanding-gnns/) for an interactive introduction to graph neural networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here, we will implement a simpler version of [NequIP](https://www.nature.com/articles/s41467-022-29939-5), one of the most popular $E(3)$-equivariant message-passing networks. The operation of the network is shown in the figure below. If this doesn't make sense to you, don't worry! We will explain the details in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](images/algorithm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's discuss [Flax](https://github.com/google/flax), the JAX framework that we will use to implement the network. Flax allows us to define neural networks in JAX, in a way similar to PyTorch.\n",
    "\n",
    "Let's see how we can define a multi-layer perceptron (MLP) in Flax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"A simple multi-layer perceptron.\"\"\"\n",
    "\n",
    "    # These are attributes.\n",
    "    output_dims: int\n",
    "    hidden_dims: int\n",
    "    num_layers: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        for _ in range(self.num_layers - 1):\n",
    "            x = nn.Dense(features=self.hidden_dims)(x)\n",
    "            x = nn.LayerNorm()(x)\n",
    "            x = nn.silu(x)\n",
    "        x = nn.Dense(features=self.output_dims)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*@nn.compact* is a decorator that allows us to define submodules (eg. nn.Dense, nn.LayerNorm above) within the __call__ method.\n",
    "\n",
    "Another thing that you may notice is the automatic shape inference: we didn't need to define the input dimensions of 'x' above. \n",
    "\n",
    "We can create a model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(output_dims=5, hidden_dims=8, num_layers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX is a functional language, which means that we need to explicitly pass in the parameters to the model when calling it. This is different from PyTorch, where the parameters are stored in the model.\n",
    "\n",
    "Fortunately, Flax provides a convenient way to handle this. We can use the 'init' method to initialize the model and the 'apply' method to call it.\n",
    "\n",
    "JAX is also explicit about randomness. We need to pass in a PRNGKey to the model when calling it, which is a seed for the random number generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.ones((3, 4))\n",
    "rng = jax.random.PRNGKey(0)\n",
    "\n",
    "# Initialize the model.\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "params = model.init(init_rng, x)\n",
    "\n",
    "# Flax provides a convenient way to print the model structure,\n",
    "# inputs and outputs.\n",
    "print(nn.tabulate(model, init_rng)(x))\n",
    "\n",
    "# The forward pass is just obtained by calling model.apply.\n",
    "y = model.apply(params, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not go too much into the details of Flax here, but you can find more information in the [Flax documentation](https://flax.readthedocs.io/en/latest/) with some cool [examples](https://flax.readthedocs.io/en/latest/examples/index.html).\n",
    "\n",
    "Let's move on to the implementation of the E(3)-equivariant message-passing network.\n",
    "\n",
    "Initially, each node gets a feature vector based on the atom type. The 'AtomEmbedding' class below implements this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtomEmbedding(nn.Module):\n",
    "    \"\"\"Embeds atomic atomic_numbers into a learnable vector space.\"\"\"\n",
    "\n",
    "    embed_dims: int\n",
    "    max_atomic_number: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, atomic_numbers: jnp.ndarray) -> jnp.ndarray:\n",
    "        atom_embeddings = nn.Embed(\n",
    "            num_embeddings=self.max_atomic_number, features=self.embed_dims\n",
    "        )(atomic_numbers)\n",
    "        return e3nn.IrrepsArray(f\"{self.embed_dims}x0e\", atom_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the feature vectors of each node are initially scalars, because the atom types (and hence, their embeddings) are invariant under rotations.\n",
    "\n",
    "The next step is to convolve the feature vectors of the neighbors of each node to get a new feature vector for each node. This is done multiple times, so we define a single 'SimpleNetworkLayer' that we can use multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNetworkLayer(nn.Module):\n",
    "    \"\"\"A layer of a simple E(3)-equivariant message passing network.\"\"\"\n",
    "\n",
    "    mlp_hidden_dims: int\n",
    "    mlp_num_layers: int\n",
    "    output_irreps: e3nn.Irreps\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(\n",
    "        self,\n",
    "        node_features: e3nn.IrrepsArray,\n",
    "        senders: jnp.ndarray,\n",
    "        receivers: jnp.ndarray,\n",
    "        relative_vectors_sh: e3nn.IrrepsArray,\n",
    "        relative_vectors_norm: jnp.ndarray,\n",
    "    ) -> e3nn.IrrepsArray:\n",
    "        \n",
    "        # Compute the skip connection.\n",
    "        node_features_skip = node_features\n",
    "\n",
    "        # Tensor product of the relative vectors and the neighbouring node features.\n",
    "        node_features_broadcasted = node_features[senders]\n",
    "        node_features_broadcasted = e3nn.tensor_product(\n",
    "            relative_vectors_sh, node_features_broadcasted\n",
    "        )\n",
    "\n",
    "        # Simply multiply each irrep by a learned scalar, based on the norm of the relative vector.\n",
    "        scalars = MLP(\n",
    "            output_dims=node_features_broadcasted.irreps.num_irreps,\n",
    "            hidden_dims=self.mlp_hidden_dims,\n",
    "            num_layers=self.mlp_num_layers,\n",
    "        )(relative_vectors_norm)\n",
    "        scalars = e3nn.IrrepsArray(f\"{scalars.shape[-1]}x0e\", scalars)\n",
    "        node_features_broadcasted = jax.vmap(lambda scale, feature: scale * feature)(\n",
    "            scalars, node_features_broadcasted\n",
    "        )\n",
    "\n",
    "        # Aggregate the node features back.\n",
    "        node_features = e3nn.scatter_mean(\n",
    "            node_features_broadcasted,\n",
    "            dst=receivers,\n",
    "            output_size=node_features.shape[0],\n",
    "        )\n",
    "\n",
    "        # Apply a non-linearity.\n",
    "        # Note that using an unnormalized non-linearity will make the model not equivariant.\n",
    "        gate_irreps = e3nn.Irreps(f\"{node_features.irreps.num_irreps}x0e\")\n",
    "        node_features_expanded = e3nn.flax.Linear(node_features.irreps + gate_irreps)(\n",
    "            node_features\n",
    "        )\n",
    "        node_features = e3nn.gate(node_features_expanded)\n",
    "\n",
    "        # Add the skip connection.\n",
    "        node_features = e3nn.concatenate([node_features, node_features_skip])\n",
    "\n",
    "        # Apply a linear transformation to the output.\n",
    "        node_features = e3nn.flax.Linear(self.output_irreps)(node_features)\n",
    "        return node_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The message-passing is implemented by keeping track of senders and receiver nodes. This is a sparse representation of the adjacency matrix of the graph! The features of each neighbor are then 'tensor product'ed with the spherical harmonics of the relative position of the sender and receiver nodes. This creates higher-order irreps!\n",
    "\n",
    "* Since we use $O(3)$-equivariant tensor products and non-linearities, the network is equivariant to *rotations*.\n",
    "\n",
    "* To account for *translation symmetry*, we only use relative positions of the neighbors. This guarantees that the network is invariant to translations.\n",
    "\n",
    "* To account for *permutation symmetry* amongst the neighbors, we sum the feature vectors of the neighbors and only then apply a non-linearity.\n",
    "\n",
    "Fortunately, most of these operations are already implemented in e3nn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features_of_relative_vectors(\n",
    "    relative_vectors: jnp.ndarray, lmax: int\n",
    ") -> Tuple[e3nn.IrrepsArray, jnp.ndarray]:\n",
    "    \"\"\"Compute the spherical harmonics of the relative vectors and their norms.\"\"\"\n",
    "    relative_vectors_sh = e3nn.spherical_harmonics(\n",
    "        e3nn.s2_irreps(lmax=lmax),\n",
    "        relative_vectors,\n",
    "        normalize=True,\n",
    "        normalization=\"norm\",\n",
    "    )\n",
    "    relative_vectors_norm = jnp.linalg.norm(relative_vectors, axis=-1, keepdims=True)\n",
    "    return relative_vectors_sh, relative_vectors_norm\n",
    "\n",
    "\n",
    "class SimpleNetwork(nn.Module):\n",
    "    \"\"\"A simple E(3)-equivariant message passing network.\"\"\"\n",
    "\n",
    "    sh_lmax: int\n",
    "    init_embed_dims: int\n",
    "    max_atomic_number: int\n",
    "    mlp_hidden_dims: int\n",
    "    mlp_num_layers: int\n",
    "    output_irreps_per_layer: Sequence[e3nn.Irreps]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, graphs: jraph.GraphsTuple) -> jnp.ndarray:\n",
    "        # Node features are initially the atomic numbers embedded.\n",
    "        node_features = graphs.nodes[\"atomic_numbers\"]\n",
    "        node_features = AtomEmbedding(\n",
    "            embed_dims=self.init_embed_dims,\n",
    "            max_atomic_number=self.max_atomic_number,\n",
    "        )(node_features)\n",
    "\n",
    "        # Precompute the spherical harmonics of the relative vectors.\n",
    "        positions = graphs.nodes[\"positions\"]\n",
    "        relative_vectors = positions[graphs.receivers] - positions[graphs.senders]\n",
    "        (\n",
    "            relative_vectors_sh,\n",
    "            relative_vectors_norm,\n",
    "        ) = compute_features_of_relative_vectors(\n",
    "            relative_vectors,\n",
    "            lmax=self.sh_lmax,\n",
    "        )\n",
    "\n",
    "        # Message passing.\n",
    "        for output_irreps in self.output_irreps_per_layer:\n",
    "            node_features = SimpleNetworkLayer(\n",
    "                mlp_hidden_dims=self.mlp_hidden_dims,\n",
    "                mlp_num_layers=self.mlp_num_layers,\n",
    "                output_irreps=output_irreps,\n",
    "            )(\n",
    "                node_features,\n",
    "                graphs.senders,\n",
    "                graphs.receivers,\n",
    "                relative_vectors_sh,\n",
    "                relative_vectors_norm,\n",
    "            )\n",
    "\n",
    "        # Readout.\n",
    "        return node_features, e3nn.scatter_mean(node_features, nel=graphs.n_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we readout the graph-level representation by summing the feature vectors of all nodes, which accounts for the permutation symmetry of the nodes.\n",
    "\n",
    "In conclusion, we have implemented a simple $E(3)$-equivariant message-passing network that operates on a molecular graph. The differences with general message-passing networks are the use of equivariant features, but otherwise, the operations are quite similar.\n",
    "\n",
    "We end with some visualizations of $E(3)$-equivariant features from a randomly initialized $E(3)$-equivariant network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNetwork(\n",
    "    sh_lmax=3,\n",
    "    init_embed_dims=8,\n",
    "    max_atomic_number=10,\n",
    "    mlp_hidden_dims=8,\n",
    "    mlp_num_layers=2,\n",
    "    output_irreps_per_layer=[\n",
    "        \"8x0e + 4x1o\",\n",
    "        \"8x0e + 4x1o\",\n",
    "        \"8x0e + 4x1o\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Initialize the model.\n",
    "params = model.init(init_rng, graph)\n",
    "\n",
    "# Jit the forward pass.\n",
    "node_features, global_features = jax.jit(model.apply)(params, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_graph(graph)\n",
    "\n",
    "# Draw the vector node features.\n",
    "vector_node_features = node_features.filter(\"1o\")\n",
    "# This is just a fancy reshape.\n",
    "vector_node_features = vector_node_features.mul_to_axis()\n",
    "\n",
    "for index, (position, vectors) in enumerate(zip(positions, vector_node_features)):\n",
    "    for vector_index, vector in enumerate(vectors):\n",
    "        scaled_vector = (vector / jnp.linalg.norm(vector.array)).array\n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                x=[position[0], position[0] + scaled_vector[0]],\n",
    "                y=[position[1], position[1] + scaled_vector[1]],\n",
    "                z=[position[2], position[2] + scaled_vector[2]],\n",
    "                mode=\"lines\",\n",
    "                line=dict(color=\"red\"),\n",
    "                showlegend=(vector_index == 0 and index == 0),\n",
    "                name=\"Vector Features\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "fig.update_layout(title=\"3D Graph with Vector Features\")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "symphony",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
